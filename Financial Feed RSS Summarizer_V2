from urllib import request
from bs4 import BeautifulSoup as bs
import re
import nltk
import heapq
import feedparser


def parse_feed(url):
    NewsFeed = feedparser.parse(url)
    entry = NewsFeed.entries
    return NewsFeed


feed = parse_feed('https://www.cnbc.com/id/10000664/device/rss/rss.html')

url = 'https://www.cnbc.com/2020/01/18/amazon-reportedly-wants-to-turn-your-hand-into-a-credit-card.html'

allParagaphContent = ''

htmlDoc = request.urlopen(url)

soupObject = bs(htmlDoc, 'html.parser')

paragraphsContents = soupObject.findAll('p')

#print(paragraphsContents)

# print(paragraphsContents)

for paragaphContent in paragraphsContents:
    allParagaphContent += (paragaphContent.text + ' ')


allParagaphContent_CleanerData = re.sub(r'\[[0-9]*\]', ' ', allParagaphContent)
allParagaphContent_CleanedData = re.sub(r'\s+', ' ', allParagaphContent_CleanerData)

tokens_sentences = nltk.sent_tokenize(allParagaphContent_CleanedData)

allParagaphContent_CleanedData = re.sub(r'[^a-zA-Z]', ' ', allParagaphContent_CleanedData)
allParagaphContent_CleanedData = re.sub(r'\s+', ' ', allParagaphContent_CleanedData)


tokens_word = nltk.sent_tokenize(allParagaphContent_CleanedData)

#print(tokens_sentences)
#Creating Sentence Token

def sent_tokensize(self, strings):
    return [self.tokensize(allParagaphContent_CleanedData) for allParagaphContent in strings]
    nltk.sent_tokensize(allParagaphContent_CleanedData)


words_tokens = nltk.word_tokenize(allParagaphContent_CleanedData)





# Calculate the Frequency

stopwords = nltk.corpus.stopwords.words('english')


word_frequencies = {}

print(tokens_word[0])

def sentence_tokenizer(sentence):
    print(type(sentence))
    clean_1 = re.sub(r'\[[0-9]*\]', ' ', sentence.lower())
    clean_2 = re.sub(r'[^a-zA-Z]', ' ', clean_1)
    clean_3 = re.sub(r'\s+', ' ', clean_2)
    clean_4 = nltk.word_tokenize(clean_3)
    return clean_4



sentence_tokenizer()


for word in words_tokens:
    if word not in stopwords:
        if word in word_frequencies.keys():
            word_frequencies[word] += 1
        else:
            word_frequencies[word] = 1


#calculate weightest frequency

maximum_frequency_word = max(word_frequencies.values())

#print(maximum_frequency_word)


#print(word_frequencies)


#if sentence not in sentences_scores.keys():
# calculate Sentence score with each word weighted frequency

sentences_scores= {}
#print(tokens_word)
#print(tokens_sentences)


#for sentence in tokens_sentences:
#    print(sentence)
#    tokens = tokens_sentences([sentence])




#for sentence in tokens_sentences:
 #   for word in nltk.word_tokenize(sentence):#Array mit getrennten w√∂rtern
  #      if word in word_frequencies.keys():
   #         if len(sentence.split(' ')) < 30:
    #            if sentence not in sentences_scores.keys():
     #               sentences_scores[sentence] = word_frequencies[word]
      #          else:
       #             sentences_scores[sentence] += word_frequencies[word]

#print(sentences_scores) #second learn

summary_machineLearning = heapq.nlargest(10, sentences_scores, key=sentences_scores.get)

#print(summary_machineLearning)

