from itertools import count
from urllib import request
from bs4 import BeautifulSoup as bs
import re
import nltk
import heapq
import feedparser


def parse_feed(url):
    NewsFeed = feedparser.parse(url)
    entry = NewsFeed.entries
    return NewsFeed


feed = parse_feed('https://www.cnbc.com/id/10000664/device/rss/rss.html')

url = 'https://www.cnbc.com/2020/01/18/amazon-reportedly-wants-to-turn-your-hand-into-a-credit-card.html'

allParagaphContent = ''

htmlDoc = request.urlopen(url)

soupObject = bs(htmlDoc, 'html.parser')

paragraphsContents = soupObject.findAll('p')

#print(paragraphsContents)

# print(paragraphsContents)

for paragaphContent in paragraphsContents:
    allParagaphContent += (paragaphContent.text + ' ')



allParagaphContent_CleanerData = re.sub(r'\[[0-9]*\]', ' ', allParagaphContent)
allParagaphContent_CleanedData = re.sub(r'\s+', ' ', allParagaphContent_CleanerData)

tokens_sentences = nltk.sent_tokenize(allParagaphContent_CleanedData)

allParagaphContent_CleanedData = re.sub(r'[^a-zA-Z]', ' ', allParagaphContent_CleanedData)
allParagaphContent_CleanedData = re.sub(r'\s+', ' ', allParagaphContent_CleanedData)

#print(type(allParagaphContent_CleanedData))


tokens_word = nltk.sent_tokenize(allParagaphContent_CleanedData)

#print(type(tokens_word))

#print(tokens_sentences)
#Creating Sentence Token

def sent_tokensize(self, strings):
    return [self.tokensize(allParagaphContent_CleanedData) for allParagaphContent in strings]
    nltk.sent_tokensize(allParagaphContent_CleanedData)


words_tokens = nltk.word_tokenize(allParagaphContent_CleanedData)


#print(type(words_tokens))


# Calculate the Frequency

stopwords = nltk.corpus.stopwords.words('english')


word_frequencies = {}

#print(type(tokens_sentences[0]))


def sentence_tokenizer(sentence):
    #print(type(list_tokenizer[]))
    clean_1 = re.sub(r'\[[0-9]*\]', ' ', sentence)#.lower())
    clean_2 = re.sub(r'[^a-zA-Z]', ' ', clean_1)
    clean_3 = re.sub(r'\s+', ' ', clean_2)
    clean_4 = nltk.word_tokenize(clean_3)
    return clean_4

#print(sentence_tokenizer(tokens_sentences[0]))


for word in words_tokens:
    if word not in stopwords:
        if word in word_frequencies.keys():
            word_frequencies[word] += 1
        else:
            word_frequencies[word] = 1


#calculate weightest frequency

maximum_frequency_word = max(word_frequencies.values())

#print(maximum_frequency_word)


print(word_frequencies)


#if sentence not in sentences_scores.keys():
# calculate Sentence score with each word weighted frequency

sentences_scores= {}
#print(tokens_word)
#print(tokens_sentences)


#for sentence in tokens_sentences:
 #   print(sentence)
  #  tokens = sentence_tokenizer([sentence])
   # if word not in stopwords:
    #    if word in word_frequencies.keys():
     #       word_frequencies[word] += 1
      #  else:
       #     word_frequencies[word] = 1


for sentence in tokens_sentences:
    #print(sentence)
    tokens = sentence_tokenizer(sentence)
    clean_tokens = [word for word in tokens if word not in stopwords]
    length=len(clean_tokens)
    value=0
    for word in clean_tokens:
        if word in word_frequencies:
            value = value + word_frequencies[word]
        else:
            value = 0

    print(value)







#for sentence in tokens_sentences:
 #   for word in nltk.word_tokenize(sentence):#Array mit getrennten w√∂rtern
  #      if word in word_frequencies.keys():
   #         if len(sentence.split(' ')) < 30:
    #            if sentence not in sentences_scores.keys():
     #               sentences_scores[sentence] = word_frequencies[word]
      #          else:
       #             sentences_scores[sentence] += word_frequencies[word]

#print(sentences_scores) #second learn

summary_machineLearning = heapq.nlargest(10, sentences_scores, key=sentences_scores.get)

#print(summary_machineLearning)

